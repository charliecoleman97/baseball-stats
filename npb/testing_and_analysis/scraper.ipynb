{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook for testingbest ways to scrape batting and pitching stats page\n",
    "#### Testing this with 2022 NPB teams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from requests_html import HTMLSession\n",
    "import pandas as pd\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sleep(seconds: int): \n",
    "    \"\"\"\n",
    "    sleep timer that will sleep for random intervals \n",
    "    between 0 and the inputted seconds. Needs to be randomised \n",
    "    to prevent timeouts on the website\n",
    "\n",
    "    :param seconds: max amount of seconds to sleep for\n",
    "    :return time.sleep: time.sleep function with randomised sleep interval\n",
    "    \"\"\"\n",
    "    return time.sleep(random.randint(0, seconds))\n",
    "\n",
    "def get_league_url(base_url: str, league: str) -> str:\n",
    "    \"\"\"\n",
    "    Finds the league page url \n",
    "\n",
    "    :param base_url: url for the baseball reference homepage\n",
    "    :param league: the name of the league we want the url for\n",
    "    :return league_url: full league url\n",
    "    \"\"\"\n",
    "    leagues_url = base_url + '/register/'\n",
    "\n",
    "    s = HTMLSession()\n",
    "    page = s.get(leagues_url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    league_link = soup.find('a', text=league)['href']\n",
    "\n",
    "    return base_url + league_link\n",
    "\n",
    "\n",
    "def get_team_urls(base_url: str, league_url: str, year=2022) -> dict:\n",
    "    \"\"\"\n",
    "    Finds the urls for each team page and saves to a dictionary \n",
    "\n",
    "    :param base_url: url for the baseball reference homepage\n",
    "    :param league_url: full url for the league page\n",
    "    :return team_dict: dictionary with team name as the key and \n",
    "                        team page url as the value \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # TO DO: Add a year variable rather than hard code 2022\n",
    "\n",
    "    s = HTMLSession()\n",
    "    page = s.get(league_url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "    # Find start and end links so we can get all the links in between\n",
    "    start_link = soup.find_all('a', text=f\"{year}\")[-2]\n",
    "    end_link = soup.find_all('a', text=f\"{year-1}\")[-2]\n",
    "\n",
    "    team_dict = {}\n",
    "    current_link = start_link.find_next('a') \n",
    "    while current_link != end_link:\n",
    "        team_name = current_link.text  # loop through links until the ending link is reached\n",
    "        full_url = base_url + current_link['href']\n",
    "        team_dict[team_name] = full_url\n",
    "        current_link = current_link.find_next('a')\n",
    "    \n",
    "    return team_dict\n",
    "\n",
    "\n",
    "def get_stats_table(url: str, team: str, id: str) -> None:  \n",
    "    \"\"\"\n",
    "    Scrapes the pitching and batting table depending on the ID provided\n",
    "    \n",
    "    :param url: full team url \n",
    "    :param id: used to determine which table to scrape (batting or pitching)\n",
    "    \"\"\"\n",
    "    \n",
    "    s = HTMLSession()\n",
    "    page = s.get(url)\n",
    "    stats_div = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "    # Pitching table is in a comment for some reason so have to do some \n",
    "    # cleaning before converting to DF\n",
    "    if id == \"team_pitching\":\n",
    "        s = str(stats_div.find(\"div\", id=\"all_team_pitching\"))\n",
    "        start_len, end_len = s.find('<table'), s.rfind('</table>')\n",
    "        cleaned_page = s[start_len:end_len + len('</table>')]\n",
    "        stats_div = BeautifulSoup(cleaned_page, \"html.parser\")  \n",
    "        \n",
    "    stats_table = stats_div.find(\"table\", id=id)\n",
    "\n",
    "    stats_df = pd.DataFrame()\n",
    "    for row in stats_table.tbody.find_all(\"tr\"):\n",
    "        columns = row.find_all(\"td\")\n",
    "        \n",
    "        if(columns != []):\n",
    "            stats_dict = {}\n",
    "            for i in range(0, len(columns)):\n",
    "                stats_dict[columns[i][\"data-stat\"]] = columns[i].text.strip()\n",
    "            stats_df = stats_df.append(stats_dict, ignore_index=True)\n",
    "\n",
    "    # Creating a local file name to export to\n",
    "    team_norm = team.lower()\n",
    "    team_norm = team_norm.replace(\" \", \"_\")\n",
    "    local_filename = f\"{team_norm}_{id}.csv\"\n",
    "\n",
    "    stats_df.to_csv(f\"resources/{local_filename}\")\n",
    "\n",
    "    return \n",
    "\n",
    "\n",
    "def main(base_url):\n",
    "\n",
    "    # Get league url for each league\n",
    "    jpl_url = get_league_url(base_url, \"Japan Pacific League\")\n",
    "    jcl_url = get_league_url(base_url, \"Japan Central League\")\n",
    "    sleep(3)\n",
    "\n",
    "    # Get team url and save as dict\n",
    "    pacfic_teams_dict = get_team_urls(base_url, jpl_url)\n",
    "    central_teams_dict = get_team_urls(base_url, jcl_url)\n",
    "    sleep(3)\n",
    "\n",
    "    # Loop through pacific teams and save result locally to CSV\n",
    "    for team in pacfic_teams_dict:\n",
    "        print(team)\n",
    "        batting_stats = get_stats_table(pacfic_teams_dict[team], team, \"team_batting\")\n",
    "        sleep(3)\n",
    "        pitching_stats = get_stats_table(pacfic_teams_dict[team], team, \"team_pitching\")\n",
    "        sleep(3)\n",
    "        print(\"*\"*10)\n",
    "\n",
    "    # Loop through central teams and save result locally to CSV\n",
    "    for team in central_teams_dict:\n",
    "        print(team)\n",
    "        batting_stats = get_stats_table(central_teams_dict[team], team, \"team_batting\")\n",
    "        sleep(3)\n",
    "        pitching_stats = get_stats_table(central_teams_dict[team], team, \"team_pitching\")\n",
    "        sleep(3)\n",
    "        print(\"*\"*10)\n",
    "    \n",
    "    return \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chiba Lotte Marines\n",
      "**********\n",
      "Fukuoka Softbank Hawks\n",
      "**********\n",
      "Hokkaido Nippon Ham Fighters\n",
      "**********\n",
      "Orix Buffaloes\n",
      "**********\n",
      "Saitama Seibu Lions\n",
      "**********\n",
      "Tohoku Rakuten Golden Eagles\n",
      "**********\n",
      "Chunichi Dragons\n",
      "**********\n",
      "Hanshin Tigers\n",
      "**********\n",
      "Hiroshima Carp\n",
      "**********\n",
      "Yakult Swallows\n",
      "**********\n",
      "Yokohama Bay Stars\n",
      "**********\n",
      "Yomiuri Giants\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://www.baseball-reference.com\" \n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main(base_url)\n",
    "    except Exception:\n",
    "        print(Exception)\n",
    "        raise"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ed17212fccccbb12e83c24386c5ff03117b85ebaf90e831a0bd1be0b92eeb92c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
